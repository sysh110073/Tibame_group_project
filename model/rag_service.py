import os
import time
import urllib.request
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_pinecone import PineconeVectorStore
from pinecone import Pinecone, ServerlessSpec
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.prompts import PromptTemplate
from langchain_classic.chains.conversational_retrieval.base import ConversationalRetrievalChain
import configparser

# è®€å– Config
config = configparser.ConfigParser()
config.read('config.ini')

# è¨­å®šç’°å¢ƒè®Šæ•¸
os.environ["GOOGLE_API_KEY"] = config['GOOGLE']['GEMINI_API_KEY']
os.environ["PINECONE_API_KEY"] = config['PINECONE']['API_KEY']

# å…¨åŸŸè®Šæ•¸
qa_chain = None

def init_rag_system():
    global qa_chain
    try:
        print("ğŸš€ åˆå§‹åŒ– RAG ç³»çµ±ä¸­...")
        
        # 1. è¨­å®š Embeddings (ä½¿ç”¨ HuggingFace é–‹æºæ¨¡å‹ï¼Œå…è²»ä¸”æ•ˆæœä¸éŒ¯)
        embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
        
        # 2. é€£ç·š Pinecone
        pc = Pinecone(api_key=os.environ.get("PINECONE_API_KEY"))
        index_name = "zero-waste-chef-recipes" 

        # 3. æª¢æŸ¥ä¸¦å»ºç«‹ Index
        if index_name not in pc.list_indexes().names():
            print(f"ğŸ“¦ ç´¢å¼• {index_name} ä¸å­˜åœ¨ï¼Œæ­£åœ¨å»ºç«‹ä¸­...")
            pc.create_index(
                name=index_name,
                dimension=384, # all-MiniLM-L6-v2 çš„ç¶­åº¦æ˜¯ 384
                metric="cosine",
                spec=ServerlessSpec(cloud="aws", region="us-east-1")
            )
            while not pc.describe_index(index_name).status['ready']:
                time.sleep(1)

        index = pc.Index(index_name)
        
        # 4. æª¢æŸ¥æ˜¯å¦éœ€è¦ä¸‹è¼‰ä¸¦ä¸Šå‚³è³‡æ–™ (Sharp é£Ÿè­œ)
        if index.describe_index_stats()['total_vector_count'] == 0:
            print("ğŸ“¥ é›²ç«¯è³‡æ–™åº«ç‚ºç©ºï¼Œé–‹å§‹ä¸‹è¼‰ é£Ÿè­œ PDF...")
            
            pdf_filename = "sharp_recipes.pdf"

            # å¦‚æœæª”æ¡ˆä¸åœ¨ï¼Œæ‰å˜—è©¦ä¸‹è¼‰
            if not os.path.exists(pdf_filename):
                print("âš ï¸ æ‰¾ä¸åˆ°æœ¬åœ° PDFï¼Œå˜—è©¦å¾ç¶²è·¯ä¸‹è¼‰...")
                # (ä½¿ç”¨ requestsä¸‹è¼‰)
                import requests
                pdf_url = "https://tw.sharp/sites/default/files/products/documents/KN-V24AT%E9%A3%9F%E8%AD%9C%E9%9B%86.pdf"
                response = requests.get(pdf_url)
                if response.status_code == 200:
                    with open(pdf_filename, 'wb') as f:
                        f.write(response.content)
                    print("âœ… ä¸‹è¼‰æˆåŠŸ")
                else:
                    raise Exception("PDF ä¸‹è¼‰å¤±æ•—ï¼Œè«‹æ‰‹å‹•ä¸‹è¼‰ 'sharp_recipes.pdf' æ”¾åˆ°è³‡æ–™å¤¾ä¸­")
            else:
                print("âœ… åµæ¸¬åˆ°æœ¬åœ° PDF æª”æ¡ˆï¼Œç›´æ¥ä½¿ç”¨ã€‚")
    

            print("ğŸ“„ é–‹å§‹è§£æ PDF...")
            loader = PyPDFLoader(pdf_filename)
            print("ğŸ“„ PDF ä¸‹è¼‰å®Œæˆï¼Œé–‹å§‹è§£æèˆ‡åˆ‡å‰²...")
            loader = PyPDFLoader(pdf_filename)
            docs = loader.load()
            
            # åˆ‡å‰²æ–‡æœ¬ï¼šé£Ÿè­œé€šå¸¸æ¯”è¼ƒçŸ­ï¼Œchunk_size è¨­ 500-800 æ•ˆæœè¼ƒå¥½
            text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)
            texts = text_splitter.split_documents(docs)
            
            print(f"ğŸ§© å…±åˆ‡å‰²æˆ {len(texts)} å€‹ç‰‡æ®µï¼Œæ­£åœ¨ä¸Šå‚³è‡³ Pinecone...")
            PineconeVectorStore.from_documents(texts, embeddings, index_name=index_name)
            print("âœ… è³‡æ–™ä¸Šå‚³å®Œç•¢ï¼")
        
        # 5. å»ºç«‹ Retriever èˆ‡ LLM
        vector_store = PineconeVectorStore.from_existing_index(index_name, embeddings)
        # k=3 ä»£è¡¨æ¯æ¬¡æœå°‹æ‰¾å› 3 å€‹æœ€ç›¸é—œçš„é£Ÿè­œç‰‡æ®µ
        retriever = vector_store.as_retriever(search_kwargs={"k": 3})
        
        # æ³¨æ„ï¼šç›®å‰ Google ç©©å®šç‰ˆæ˜¯ gemini-1.5-flashï¼Œè‹¥ç„¡ 2.5 æ¬Šé™è«‹æ”¹å› 1.5
        llm = ChatGoogleGenerativeAI(model="gemini-2.5-flash", temperature=0.7)

        # 6. è¨­å®š Prompt Template (è§’è‰²æ‰®æ¼”)
        custom_template = """
        ä½ æ˜¯ã€Œé›¶å‰©é£Ÿ AI å‰µæ„ä¸»å»šã€ï¼Œè‡´åŠ›æ–¼å¹«åŠ©ä½¿ç”¨è€…åˆ©ç”¨å†°ç®±å‰©é£Ÿåšå‡ºç¾å‘³æ–™ç†ã€‚
        è«‹æ ¹æ“šä¸‹æ–¹çš„ã€åƒè€ƒé£Ÿè­œè³‡æ–™åº«ã€‘ï¼ˆä¾†æºï¼šSharp æ™ºæ…§é‹é£Ÿè­œï¼‰ä¾†å›ç­”ç”¨æˆ¶çš„å•é¡Œã€‚
        
        è¦å‰‡ï¼š
        1. å¦‚æœã€åƒè€ƒé£Ÿè­œè³‡æ–™åº«ã€‘ä¸­æœ‰åˆé©çš„é£Ÿè­œï¼Œè«‹å„ªå…ˆåƒè€ƒå…¶èª¿å‘³æ¯”ä¾‹èˆ‡çƒ¹é£ªæ™‚é–“ã€‚
        2. å¦‚æœè³‡æ–™åº«ä¸­æ²’æœ‰å®Œå…¨åŒ¹é…çš„èœè‰²ï¼Œè«‹é‹ç”¨ä½ èº«ç‚º AI ä¸»å»šçš„çŸ¥è­˜é€²è¡Œå‰µæ„æ”¹è‰¯ï¼Œä½†è«‹èªªæ˜é€™æ˜¯ä½ çš„å»ºè­°ã€‚
        3. å›ç­”æ™‚è«‹ä¿æŒèªæ°£è¦ªåˆ‡ã€é¼“å‹µç’°ä¿ã€‚
        
        ã€åƒè€ƒé£Ÿè­œè³‡æ–™åº«ã€‘ï¼š
        {context}
        
        ç”¨æˆ¶å‰©é£Ÿ/éœ€æ±‚ï¼š{question}
        ä¸»å»šå»ºè­°å›ç­”ï¼š
        """
        PROMPT = PromptTemplate(template=custom_template, input_variables=["context", "question"])

        # 7. å»ºç«‹ Chain
        qa_chain = ConversationalRetrievalChain.from_llm(
            llm=llm,
            retriever=retriever,
            return_source_documents=True,
            combine_docs_chain_kwargs={"prompt": PROMPT}
        )
        print("âœ… AI ä¸»å»šç³»çµ± (RAG) æº–å‚™å°±ç·’ï¼")

    except Exception as e:
        print(f"âŒ RAG åˆå§‹åŒ–å¤±æ•—: {e}")

# é€éé€™å€‹å‡½æ•¸çµ¦å¤–éƒ¨å‘¼å«
def get_chef_response(user_input, chat_history=[]):
    if qa_chain is None:
        return "ç³»çµ±æ­£åœ¨æš–æ©Ÿä¸­ï¼Œè«‹ç¨å¾Œå†è©¦..."
    
    # invoke çš„è¼¸å…¥å¿…é ˆåŒ…å« question èˆ‡ chat_history
    result = qa_chain.invoke({"question": user_input, "chat_history": chat_history})
    return result["answer"]

# åœ¨æ¨¡çµ„è¼‰å…¥æ™‚è‡ªå‹•åŸ·è¡Œåˆå§‹åŒ–
init_rag_system()